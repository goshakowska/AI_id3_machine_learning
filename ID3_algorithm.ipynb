{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ćwiczenie 4 - Regresja i klasyfikacja\n",
    "## Implementacja drzew decyzyjnych tworzonych algorytmem ID3 z ograniczeniem maksymalnej głębokości drzewa.\n",
    "Ćwiczenie wykonała Małgorzata Kozłowska, 3186810"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importowanie modułów\n",
    "- potrzebnych do wykonania ćwiczenia. W realizacji algorytmu oraz jego badania korzystam z biblioteki typing, numpy, pandas,oraz sklearn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementacja metod potrzebnych do przetworzenia danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_data_from_csv_file(file_handle):\n",
    "    \"\"\"\n",
    "    Extracts data from file. Converts it into dataframe pandas type.\n",
    "    :param file_handle:\n",
    "    :return: Data represented as pandas DataFrame type.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(file_handle, header=None)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_data_for_analysis(dataset: pd.DataFrame, column_names: List[str]):\n",
    "    \"\"\"\n",
    "    Adds column names for better code and data analysis.\n",
    "    :return: Modified data with column names\n",
    "    \"\"\"\n",
    "    dataset.columns = column_names\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def divide_dataset_for_target_attribute(dataset: pd.DataFrame, target_attribute: str):\n",
    "    \"\"\"\n",
    "    Divides provided dataset into two:\n",
    "    1) contains all the attributes (and their values) on which our ID3 algorithm will learn \"the pattern\" of the data.\n",
    "    2) contains the target attribute's values, which later will be predicted by the ID3 tree\n",
    "    :param dataset: provided dataset for analysis\n",
    "    :param target_attribute: target attribute on account of which the dataset will be analysed\n",
    "    :return: two datasets (as array and column)\n",
    "    \"\"\"\n",
    "    dataset_to_analyse = dataset.drop([target_attribute], axis=1)\n",
    "    target_attribute_column = dataset[target_attribute]\n",
    "    return dataset_to_analyse, target_attribute_column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementacja metod związanych z rachunkami wykorzystywanymi w algorytmie ID3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def information_gain(dataset_entropy: float, attribute_name: str, attribute_average_information: float):\n",
    "    \"\"\"\n",
    "    Method that calculates information gain for the provided attribute of the dataset.\n",
    "    :param dataset_entropy: calculated entropy for the whole provided (earlier) dataset\n",
    "    :param attribute_name: analysed attribute\n",
    "    :param attribute_average_information: calculated earlier average information for the given attribute\n",
    "    :return: name of the attribute and its information gain\n",
    "    \"\"\"\n",
    "    information_gain = dataset_entropy - attribute_average_information\n",
    "    return attribute_name, information_gain\n",
    "\n",
    "\n",
    "def find_best_attribute(attributes_information_gain: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Method that finds the best attribute based on its information gain.\n",
    "    :param attributes_information_gain: list of information gains for each attribute in the dataset.\n",
    "    :return: attribute which provides the highest information gain.\n",
    "    \"\"\"\n",
    "    return max(attributes_information_gain, key=lambda value: attributes_information_gain[value])\n",
    "\n",
    "\n",
    "def entropy(target_attribute_values: pd.Series):\n",
    "    \"\"\"\n",
    "    Method that calculates entropy of the provided target attribute based on the formula\n",
    "    :param target_attribute_values:\n",
    "    :return: calculated entropy for the provided attribute\n",
    "    \"\"\"\n",
    "\n",
    "    class_values_categorized = target_attribute_values.value_counts()\n",
    "    total_class_number = class_values_categorized.sum()\n",
    "    entropy = sum([-(distinct_value_counts / total_class_number)*np.log2(distinct_value_counts / total_class_number)\n",
    "                   for distinct_value_counts in class_values_categorized])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_information_gains_for_each_attribute(dataset_to_analyse: pd.DataFrame, target_attribute: pd.Series):\n",
    "    \"\"\"\n",
    "    Wrapper function that calculates information gain for each attribute in provided dataset.\n",
    "    :param dataset_to_analyse:\n",
    "    :param target_attribute: target attribute in\n",
    "    :return: dictionary which stores data in the form: {key = attribute's name: value = information gain of that attribute}\n",
    "    \"\"\"\n",
    "    attribute_information_gains = {}\n",
    "    for each_column in dataset_to_analyse.columns:\n",
    "        attr_name, information = information_gain(entropy(target_attribute),\n",
    "                                                  average_information(dataset_to_analyse[each_column], target_attribute)[0],\n",
    "                                                  average_information(dataset_to_analyse[each_column], target_attribute)[1])\n",
    "        attribute_information_gains[attr_name] = information\n",
    "    return attribute_information_gains\n",
    "\n",
    "\n",
    "def average_information(data_attribute_subset: pd.DataFrame, target_attribute_values: pd.Series):\n",
    "    \"\"\"\n",
    "    Method that calculates average information obtained from the provided attribute in the dataset\n",
    "    It iterates through each value (of the given attribute) and calculates entropy of the target attribute\n",
    "    (for that attribute value).\n",
    "    :param data_attribute_subset: series of the given attribute\n",
    "    :param target_attribute_values: target attribute values\n",
    "    :return: attributes name, average information for the given attribute\n",
    "    \"\"\"\n",
    "    values_to_analyse = data_attribute_subset.unique()\n",
    "    total_class_number = data_attribute_subset.value_counts().sum()\n",
    "    attribute_average_information = 0\n",
    "\n",
    "    for each_value in values_to_analyse:\n",
    "        value_fraction = data_attribute_subset.value_counts()[each_value].sum()/total_class_number\n",
    "        selected_rows = data_attribute_subset.where(data_attribute_subset == each_value)\n",
    "        rows_to_analyse = target_attribute_values.copy().loc[selected_rows.dropna().index.values.tolist()]\n",
    "        attribute_average_information += value_fraction*entropy(rows_to_analyse)\n",
    "    return data_attribute_subset.name, attribute_average_information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementacja klasy ID3Node\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ID3Node:\n",
    "    def __init__(self, attribute_name: str, values_of_attribute, children_nodes: ['ID3Node'] = None):\n",
    "        self._attribute_name = attribute_name\n",
    "        self._tree_branches_dictionary = {value: child_node for value, child_node in zip(values_of_attribute, children_nodes)}\n",
    "        self._values_of_attribute = values_of_attribute\n",
    "        self._children_nodes = children_nodes\n",
    "\n",
    "    def predict_next_node(self, data_row: pd.Series):\n",
    "        \"\"\"\n",
    "        Method that for each provided row iterates through tree branches dictionary in order to find the value of\n",
    "        the target attribute. If the provided node does not exist (there is a Key Error) the value chosen is the most\n",
    "        frequent value from the node's children\n",
    "        :param data_row: one row of data\n",
    "        \"\"\"\n",
    "        value = data_row[self._attribute_name]\n",
    "        try:\n",
    "            next_node = self._tree_branches_dictionary[value]\n",
    "\n",
    "            if not isinstance(next_node, ID3Node):\n",
    "                return next_node\n",
    "\n",
    "        except KeyError:\n",
    "            most_frequent_target_attribute_value = []\n",
    "            for child_node in self._tree_branches_dictionary.values():\n",
    "                if not isinstance(child_node, ID3Node):\n",
    "                    most_frequent_target_attribute_value.append(child_node)\n",
    "                    continue\n",
    "                most_frequent_target_attribute_value.append(child_node.predict_next_node(data_row))\n",
    "\n",
    "            most_frequent_value = max(set(most_frequent_target_attribute_value), key=most_frequent_target_attribute_value.count)\n",
    "            return most_frequent_value\n",
    "        return next_node.predict_next_node(data_row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementacja klasy ID3Tree\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ID3Tree:\n",
    "    def __init__(self, max_depth: int):\n",
    "        self._max_depth = max_depth\n",
    "        self._root = None\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        ID3 tree maximum depth getter.\n",
    "        :return: maximum depth of the ID3 tree.\n",
    "        \"\"\"\n",
    "        return self._max_depth\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"\n",
    "        ID3 tree root getter.\n",
    "        :return: root of the ID3 tree.\n",
    "        \"\"\"\n",
    "        return self._root\n",
    "\n",
    "    def set_root(self, new_root: ID3Node):\n",
    "        \"\"\"\n",
    "        ID3 tree root setter.\n",
    "        :param new_root: ID3 Node object, which will be the root of the ID3 tree.\n",
    "        \"\"\"\n",
    "        self._root = new_root\n",
    "\n",
    "    def build_ID3_tree(self, dataset_to_analyse: pd.DataFrame, target_attribute: pd.Series, depth: int):\n",
    "        \"\"\"\n",
    "        Creates ID3 tree structure by recursively finding best attribute to divide the current set. When the given value\n",
    "        of the attribute has homogeneous target attribute values or the tree reached its maximum depth (provided by user)\n",
    "        or there are no columns (attributes) to analyse we return the leaf node\n",
    "        equal to 0,\n",
    "        :param dataset_to_analyse: provided part of the data on which the model will learn dependencies between data\n",
    "        :param target_attribute:\n",
    "        :param depth:\n",
    "        :return: an ID3 tree with newly created ID3 Nodes\n",
    "        \"\"\"\n",
    "        total_class_number = target_attribute.value_counts()\n",
    "        if len(total_class_number) == 1 or depth == 0 or len(dataset_to_analyse.columns) == 0:\n",
    "            return total_class_number.index.tolist()[0]\n",
    "        attributes_information_gain = calculate_information_gains_for_each_attribute(dataset_to_analyse, target_attribute)\n",
    "        division_attribute = find_best_attribute(attributes_information_gain)\n",
    "        values_to_analyse = dataset_to_analyse[division_attribute].unique()\n",
    "        children_nodes = [self.build_ID3_tree(dataset_to_analyse[dataset_to_analyse[division_attribute] == value].drop(division_attribute, axis=1),\n",
    "                                          target_attribute[dataset_to_analyse[division_attribute] == value], depth - 1) for value in values_to_analyse]\n",
    "        return ID3Node(division_attribute, values_to_analyse, children_nodes)\n",
    "\n",
    "    def predict_target_attribute_value(self, train_dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Method that for the provided dataset iterates through nodes (starting from the root node\n",
    "        - which is the ID3Tree instance attribute) in order to find target attribute value.\n",
    "        Predict_next_node method (which is a method of the ID3Node class) is called - it iterates\n",
    "        through each row of the provided dataset.\n",
    "        :param train_dataset: dataset to analyse\n",
    "        :return: series of the predicted target attribute values\n",
    "        \"\"\"\n",
    "        root_node = self.get_root()\n",
    "\n",
    "        return train_dataset.apply(lambda data_row: root_node.predict_next_node(data_row), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementacja metody znajdującej optymalną głębokość drzewa algorytmu ID3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def find_differences(tree: ID3Tree, validation_dataset: pd.DataFrame, validation_target: pd.Series):\n",
    "    predicted_validation_target = tree.predict_target_attribute_value(validation_dataset)\n",
    "    number_of_different_values_in_rows = validation_target.compare(predicted_validation_target).count()[0]\n",
    "    return number_of_different_values_in_rows\n",
    "\n",
    "def find_best_depth(trees_for_analysis: List[ID3Tree]):\n",
    "    depth_and_differences = {}\n",
    "    for each_tree_index in range(len(trees_for_analysis)):\n",
    "        depth_and_differences[each_tree_index + 1] = find_differences(trees_for_analysis[each_tree_index], x_valid, y_valid)\n",
    "    best_max_depth = min(depth_and_differences, key=depth_and_differences.get)\n",
    "    return best_max_depth, depth_and_differences\n",
    "\n",
    "def evaluate_best_depth(depth_and_differences: Dict):\n",
    "    best_depth, depth_and_wrong_evaluation_number = depth_and_differences\n",
    "    print(\"Najbardziej optymalna głębokość drzewa ID3 dla zbioru danych otrzymanych z pliku 'breast_cancer.csv' to: \" + str(best_depth))\n",
    "    print(\"Liczba źle skategoryzowanych danych: \" + str(depth_and_wrong_evaluation_number[best_depth]))\n",
    "\n",
    "def evaluate_depths(depths_and_differences: Dict):\n",
    "    print(\"Dla głębokości drzewa ID3 równej: \")\n",
    "    for each_key in depths_and_differences:\n",
    "        print(str(each_key) + \" - zostało źle sklasyfikowanych: \" +str(depths_and_differences[each_key]) + \" danych.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def create_trees_for_analysis(dataset: pd.DataFrame, target_attribute: pd.Series, depth_range: int):\n",
    "    list_of_trees = []\n",
    "    for each_depth in range(1, depth_range + 1):\n",
    "        new_tree = ID3Tree(each_depth)\n",
    "        new_tree.set_root(new_tree.build_ID3_tree(dataset, target_attribute, new_tree.get_max_depth()))\n",
    "        list_of_trees.append(new_tree)\n",
    "    return list_of_trees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Badanie klasyfikatorów\n",
    "Przygotowujemy dane do analizy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "data_frame = extract_data_from_csv_file('breast_cancer.csv')\n",
    "breast_cancer_columns = [\"Class\", \"age\", \"menopause\", \"tumor size\", \"inv nodes\", \"node caps\", \"deg malig\", \"breast\", \"breast quad\", \"irradiat\"]\n",
    "formatted_data = prepare_data_for_analysis(data_frame, breast_cancer_columns)\n",
    "data, target = divide_dataset_for_target_attribute(formatted_data, 'irradiat')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zbiór danych dzielimy odpowiednio na zbiory: trenujący, walidacyjny oraz testowy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "x_train, x_rem, y_train, y_rem = train_test_split(data, target, train_size=0.8)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Znajdujemy najlepszą głębokość drzewa ID3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najbardziej optymalna głębokość drzewa ID3 dla zbioru danych otrzymanych z pliku 'breast_cancer.csv' to: 2\n",
      "Liczba źle skategoryzowanych danych: 6\n",
      "Dla głębokości drzewa ID3 równej: \n",
      "1 - zostało źle sklasyfikowanych: 9 danych.\n",
      "2 - zostało źle sklasyfikowanych: 6 danych.\n",
      "3 - zostało źle sklasyfikowanych: 7 danych.\n",
      "4 - zostało źle sklasyfikowanych: 9 danych.\n",
      "5 - zostało źle sklasyfikowanych: 10 danych.\n",
      "6 - zostało źle sklasyfikowanych: 9 danych.\n"
     ]
    }
   ],
   "source": [
    "trees_for_analysis = create_trees_for_analysis(x_train, y_train, 6)\n",
    "depth_evaluation = find_best_depth(trees_for_analysis)\n",
    "best_depth, depths_and_mistakes = depth_evaluation\n",
    "evaluate_best_depth(depth_evaluation)\n",
    "evaluate_depths(depths_and_mistakes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wnioski\n",
    "\n",
    "Można zauważyć, że w drzewach ID3 pomimo zwiększania maksymalnej głębokości ewaluacji atrybutów precyzja odnajdywania odpowiedniej wartości ze zbioru atrybutów docelowych (target attributes) maleje. Dochodzi do błędnej klasyfikacji danych - mamy do czynienia ze zjawiskiem tzw. przeuczenia drzewa.\n",
    "\n",
    "Dla danych zadanych w poleceniu najlepsza głębokość to 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 12, 1: 8, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "best_depths = {}\n",
    "for _ in range(20):\n",
    "    x_train, x_rem, y_train, y_rem = train_test_split(data, target, train_size=0.8)\n",
    "    x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5)\n",
    "    trees_for_analysis = create_trees_for_analysis(x_train, y_train, 6)\n",
    "    best_depth = find_best_depth(trees_for_analysis)[0]\n",
    "    if best_depth not in best_depths.keys():\n",
    "        best_depths[best_depth] = 1\n",
    "    best_depths[best_depth] += 1\n",
    "print(best_depths)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
